from pydantic import BaseModel, Field
from yaml import serialize

from app.agent.menu_recommend.neo4j_db import Neo4jService
from app.agent.menu_recommend.tools import NaverLocalSearchTool
import operator
import os
from typing import Annotated, TypedDict, Dict, Literal
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, ToolMessage
from langgraph.graph import StateGraph, START, END
from langgraph.prebuilt import ToolNode, tools_condition
from typing import Optional, List
from langchain.messages import HumanMessage, AIMessage

from app.clients.naver import naver_map_client


class UserPreference(BaseModel):
    """ì‚¬ìš©ìì˜ ê°œë³„ ì·¨í–¥ ì •ë³´"""
    category_path: str = Field(
        description="ê³„ì¸µì  ì¹´í…Œê³ ë¦¬ ê²½ë¡œ. 'ëŒ€ë¶„ë¥˜ > ì¤‘ë¶„ë¥˜ > ì†Œë¶„ë¥˜' í˜•ì‹ (ì˜ˆ: 'ìŒì‹ > ì¼ì‹ > ì´ˆë°¥')"
    )
    preference_type: Literal["LIKES", "DISLIKES", "ALLERGIC_TO"] = Field(
        description="ì·¨í–¥ ìœ í˜•: ì¢‹ì•„í•¨(LIKES), ì‹«ì–´í•¨(DISLIKES), ì•ŒëŸ¬ì§€(ALLERGIC_TO)"
    )
    domain: Literal["FOOD", "TRAVEL", "LIFESTYLE"] = Field(
        default="FOOD",
        description="ì·¨í–¥ì˜ ë„ë©”ì¸ ë¶„ì•¼"
    )
    reason: Optional[str] = Field(
        default=None,
        description="ì·¨í–¥ì˜ ì´ìœ ë‚˜ êµ¬ì²´ì ì¸ ì„¤ëª…"
    )


class PreferenceList(BaseModel):
    """ì¶”ì¶œëœ ì·¨í–¥ ì •ë³´ì˜ ë¦¬ìŠ¤íŠ¸ ì»¨í…Œì´ë„ˆ"""
    preferences: List[UserPreference] = Field(
        description="ì‚¬ìš©ì ëŒ€í™”ì—ì„œ ì¶”ì¶œëœ ëª¨ë“  ì·¨í–¥ ì •ë³´ ë¦¬ìŠ¤íŠ¸"
    )


class UserInfo(BaseModel):
    lat: float | None = None
    lng: float | None = None


# --- State ì •ì˜ ---
class AgentState(TypedDict):
    user_id: str
    messages: Annotated[list, operator.add]
    context: str
    search_params: Dict[str, int]  # ë™ì  í˜ì´ì§•ìš©
    has_results: bool
    user_info: UserInfo
    address: Optional[str]  # ë³€í™˜ëœ ì£¼ì†Œ (ì˜ˆ: "ê°•ë™êµ¬ ì²œí˜¸ë™")
    is_related: bool  # ê°€ë“œë ˆì¼ í†µê³¼ ì—¬ë¶€ í”Œë˜ê·¸


class RelevanceCheck(BaseModel):
    """ì§ˆë¬¸ì˜ ê´€ë ¨ì„± íŒë³„ ê²°ê³¼"""
    is_related: bool = Field(description="ìŒì‹/ë§›ì§‘ ê´€ë ¨ ì§ˆë¬¸ ì—¬ë¶€")
    reason: str = Field(description="ì´ìœ ë¥¼ í•œê¸€ë¡œ ì„¤ëª… (ì˜ˆ: 'ìŒì‹ê³¼ ê´€ë ¨ ì—†ëŠ” ì¼ìƒ ëŒ€í™”ì…ë‹ˆë‹¤')")


async def guardrail(state: AgentState):
    """ì‚¬ìš©ìì˜ ë§ˆì§€ë§‰ ì§ˆë¬¸ í•˜ë‚˜ë§Œ ì¶”ì¶œí•˜ì—¬ ê´€ë ¨ì„±ì„ ì—„ê²©íˆ íŒë³„í•©ë‹ˆë‹¤."""

    # 1. ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ì—ì„œ ë§ˆì§€ë§‰ HumanMessage ê°ì²´ë§Œ ì•ˆì „í•˜ê²Œ ì¶”ì¶œ
    user_messages = [m for m in state["messages"] if isinstance(m, HumanMessage)]
    if not user_messages:
        return {"is_related": True}  # ë©”ì‹œì§€ê°€ ì—†ìœ¼ë©´ ì¼ë‹¨ í†µê³¼

    last_query = user_messages[-1].content

    # 2. LLM ì„¤ì • (í† í° ì œí•œì„ ì ì ˆíˆ ë‘ì–´ Truncation ì—ëŸ¬ ë°©ì§€)
    # max_tokensë¥¼ ë„ˆë¬´ í¬ê²Œ ì¡ìœ¼ë©´(5000 ë“±) ëª¨ë¸ì´ ë°©í™©í•  í™•ë¥ ì´ ë†’ì•„ì§‘ë‹ˆë‹¤.
    llm = ChatOpenAI(
        model="gpt-4o",
        temperature=0,
        max_tokens=500
    ).with_structured_output(RelevanceCheck)

    # 3. ë¦¬ìŠ¤íŠ¸ í˜•ì‹ì´ ì•„ë‹Œ 'ë‹¨ì¼ ì§ˆë¬¸' ê´€ì ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ ì¬êµ¬ì„±
    try:
        check = await llm.ainvoke([
            SystemMessage(content="""ë‹¹ì‹ ì€ ë§›ì§‘ ì¶”ì²œ ì„œë¹„ìŠ¤ì˜ ë³´ì•ˆ ê°€ë“œì…ë‹ˆë‹¤.
            ì‚¬ìš©ìì˜ ì…ë ¥ì´ 'ìŒì‹, ë©”ë‰´, ì‹ë‹¹, ë§›ì§‘, ìš”ë¦¬, ì·¨í–¥' ì¤‘ í•˜ë‚˜ë¼ë„ ê´€ë ¨ì´ ìˆëŠ”ì§€ íŒë³„í•˜ì„¸ìš”.

            [íŒë‹¨ ê¸°ì¤€]
            - ê´€ë ¨ ìˆìŒ(True): "ê°•ë‚¨ì—­ ë§›ì§‘ ì¶”ì²œí•´ì¤˜", "ì´ˆë°¥ ì¢‹ì•„í•´", "ë§¤ìš´ ê±´ ëª» ë¨¹ì–´", "ì•ˆë…•(ì¸ì‚¬)"
            - ê´€ë ¨ ì—†ìŒ(False): "ì˜¤ëŠ˜ ë‚ ì”¨ ì–´ë•Œ?", "ë¹„íŠ¸ì½”ì¸ ì‹œì„¸ ì•Œë ¤ì¤˜", "íŒŒì´ì¬ ì½”ë“œ ì§œì¤˜" ë“±

            ë°˜ë“œì‹œ í•œê¸€ë¡œ ì´ìœ (reason)ë¥¼ ì§§ê²Œ í¬í•¨í•˜ì—¬ JSONìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”."""),
            HumanMessage(content=f"ì‚¬ìš©ì ì§ˆë¬¸: {last_query}")  # ë¬¸ìì—´ë¡œ ê°ì‹¸ì„œ ì „ë‹¬
        ])

        # ë””ë²„ê¹… ì¶œë ¥ (ê°ì²´ í˜•íƒœ í™•ì¸)
        print(f"--- Guardrail Check ---\nQuery: {last_query}\nResult: {check}\n------------------------")

        if not check.is_related:
            rejection_msg = AIMessage(content="ğŸ• ì €ëŠ” ìŒì‹ê³¼ ë§›ì§‘ì— ëŒ€í•´ì„œë§Œ ì´ì•¼ê¸°í•  ìˆ˜ ìˆëŠ” ì „ë¬¸ê°€ì˜ˆìš”! ìŒì‹ ì·¨í–¥ì´ë‚˜ ë¨¹ê³  ì‹¶ì€ ë©”ë‰´ì— ëŒ€í•´ ë¬¼ì–´ë´ ì£¼ì‹œê² ì–´ìš”?")
            # ì—ëŸ¬ ë°©ì§€: ë¦¬ìŠ¤íŠ¸ì— ìƒˆ ë©”ì‹œì§€ë¥¼ ì¶”ê°€í•˜ê³  í”Œë˜ê·¸ ì„¤ì •
            return {"messages": [rejection_msg], "is_related": False}

        return {"is_related": True}

    except Exception as e:
        # LLM íŒŒì‹± ì—ëŸ¬ ë°œìƒ ì‹œ ì•ˆì „í•˜ê²Œ í†µê³¼ì‹œí‚¤ëŠ” Fallback ë¡œì§
        print(f"Guardrail Error: {e}")
        return {"is_related": True}


# 2. ì¡°ê±´ë¶€ ì—£ì§€ í•¨ìˆ˜
def route_after_guardrail(state: AgentState):
    """
    is_relatedê°€ Falseë©´ ë°”ë¡œ ì¢…ë£Œ(END) ì‹œê·¸ë„ì„ ë³´ëƒ…ë‹ˆë‹¤.
    """
    if state.get("is_related") is False:
        # ê´€ë ¨ ì—†ëŠ” ì§ˆë¬¸ì€ ë’¤ë„ ì•ˆ ëŒì•„ë³´ê³  END!
        return "terminate"
    return "continue"


async def resolve_location(state: AgentState):
    """ì¢Œí‘œê°€ ìˆë‹¤ë©´ ì£¼ì†Œë¡œ ë³€í™˜í•˜ì—¬ stateì— ì €ì¥"""
    lat = state["user_info"].lat
    lng = state["user_info"].lng
    if lat and lng:
        map_client = naver_map_client
        address = await map_client.get_address(lat=lat, lng=lng)
        print(address)
        return {"address": address}
    return {"address": None}


# --- ë…¸ë“œ êµ¬í˜„ ---
async def load_memories(state: AgentState):
    db = Neo4jService(os.getenv("NEO4J_URI"), os.getenv("NEO4J_USER"), os.getenv("NEO4J_PASSWORD"))
    prefs = await db.get_user_context(state["user_id"])

    formatted = "\n".join([
        f"- {' > '.join(p['category_path'])}: {p['preference_type']}" for p in prefs
    ]) if prefs else "ì·¨í–¥ ì •ë³´ ì—†ìŒ"

    print("ì·¨í–¥ ê²°ê³¼ : ", formatted)
    return {"context": formatted, "search_params": {"start": 1, "display": 5, "retry_count": 0}}


async def call_agent(state: AgentState):
    # ê¸°ì¡´ íˆ´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    llm = ChatOpenAI(model="gpt-4o", temperature=0).bind_tools([NaverLocalSearchTool()])

    # ìœ„ì¹˜ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
    loc_context = f"\n[í˜„ì¬ ìœ„ì¹˜] {state['address']}" if state.get("address") else ""

    system_msg = SystemMessage(content=f"""ë§›ì§‘ ì¶”ì²œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
    [ì‚¬ìš©ì ì·¨í–¥ ì •ë³´]
    {state['context']}
    {loc_context}

    ì§€ì¹¨:
    1. ì‚¬ìš©ìì˜ í˜„ì¬ ìœ„ì¹˜({state.get('address', 'ì•Œ ìˆ˜ ì—†ìŒ')})ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë§›ì§‘ì„ ê²€ìƒ‰í•˜ì„¸ìš”.
    2. 'naver_local_search' ë„êµ¬ë¥¼ í˜¸ì¶œí•  ë•Œ, ì¿¼ë¦¬ì— ë°˜ë“œì‹œ ì§€ì—­ëª…ê³¼ ë©”ë‰´ë¥¼ í¬í•¨í•˜ì„¸ìš”.
       ì˜ˆ: "{state.get('address', '')} ì´ˆë°¥ ë§›ì§‘"
    3. ê²°ê³¼ê°€ ë§Œì¡±ìŠ¤ëŸ½ì§€ ì•Šìœ¼ë©´ ê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ ë°”ê¿”ì„œ ì¬ì‹œë„í•˜ì„¸ìš”.""")

    response = await llm.ainvoke([system_msg] + state["messages"])
    return {"messages": [response]}


def should_continue(state: AgentState):
    """ê²€ìƒ‰ ê²°ê³¼ ìœ ë¬´ì— ë”°ë¥¸ ë£¨í”„ ê²°ì •"""
    last_msg = state["messages"][-1]

    # ë„êµ¬ ì‘ë‹µ ë©”ì‹œì§€ ì¤‘ ê²€ìƒ‰ ê²°ê³¼ê°€ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸
    tool_msgs = [m for m in state["messages"] if isinstance(m, ToolMessage) and m.name == "naver_local_search"]
    if tool_msgs:
        last_result = tool_msgs[-1].content
        if "[]" in last_result or len(last_result) < 10:
            if state["search_params"]["retry_count"] < 2:
                return "adjust_params"

    return tools_condition(state)


async def adjust_params(state: AgentState):
    curr = state["search_params"]
    return {
        "search_params": {
            "start": curr["start"] + 5,
            "display": 5,
            "retry_count": curr["retry_count"] + 1
        }
    }


async def sync_db(state: AgentState):
    """ì‚¬ìš©ìì˜ ë§ˆì§€ë§‰ ë©”ì‹œì§€ì—ì„œë§Œ ì·¨í–¥ì„ ì¶”ì¶œí•˜ì—¬ Neo4jì— ì €ì¥"""

    # 1. ë©”ì‹œì§€ ê¸°ë¡ ì¤‘ ì‚¬ìš©ìê°€ ë³´ë‚¸ ê²ƒë§Œ í•„í„°ë§
    user_messages = [m for m in state["messages"] if isinstance(m, HumanMessage)]

    # ì‚¬ìš©ìì˜ ë©”ì‹œì§€ê°€ ì—†ìœ¼ë©´(ê·¸ëŸ´ ë¦¬ ì—†ê² ì§€ë§Œ ë°©ì–´ì½”ë“œ) ì¢…ë£Œ
    if not user_messages:
        return {"messages": []}

    # ê°€ì¥ ìµœê·¼ì˜ ì‚¬ìš©ì ë©”ì‹œì§€ ì„ íƒ
    last_user_query = user_messages[-1].content

    # 2. Structured Output ì„¤ì •
    llm = ChatOpenAI(model="gpt-4o", temperature=0, max_tokens=500, ).with_structured_output(PreferenceList)

    # 3. ì¶”ì¶œ ìˆ˜í–‰ (ì‚¬ìš©ìì˜ ë°œí™” ë‚´ìš©ë§Œ ì „ë‹¬)
    extracted = await llm.ainvoke([
        SystemMessage(content="ì‚¬ìš©ìì˜ ë°œí™”ì—ì„œ ìŒì‹/ì—¬í–‰ ì·¨í–¥ì„ 'ëŒ€ë¶„ë¥˜ > ì¤‘ë¶„ë¥˜ > ì†Œë¶„ë¥˜' ê²½ë¡œë¡œ ì¶”ì¶œí•˜ì„¸ìš”. ì¶”ì²œ ê²°ê³¼ê°€ ì•„ë‹Œ ì‚¬ìš©ìì˜ ì‹¤ì œ ì„ í˜¸ë„ë§Œ ì¶”ì¶œí•´ì•¼ í•©ë‹ˆë‹¤."),
        HumanMessage(content=last_user_query)
    ])

    # 4. DB ì €ì¥
    if extracted and extracted.preferences:
        db = Neo4jService(
            os.getenv("NEO4J_URI"),
            os.getenv("NEO4J_USER"),
            os.getenv("NEO4J_PASSWORD")
        )

        for p in extracted.preferences:
            await db.upsert_hierarchical_preference(
                state["user_id"],
                p.category_path,
                p.preference_type,
                p.domain
            )

    return {"messages": []}


# --- ê·¸ë˜í”„ êµ¬ì¶• ---
workflow = StateGraph(AgentState)
workflow.add_node("resolve_location", resolve_location)
workflow.add_node("load_memories", load_memories)
workflow.add_node("agent", call_agent)
workflow.add_node("tools", ToolNode([NaverLocalSearchTool()]))
workflow.add_node("adjust_params", adjust_params)
workflow.add_node("sync_db", sync_db)
workflow.add_node("guardrail", guardrail)

workflow.add_edge(START, "guardrail")
# ê°€ë“œë ˆì¼ ê²°ê³¼ì— ë”°ë¥¸ ë¶„ê¸° ë¡œì§ ì—…ë°ì´íŠ¸
workflow.add_conditional_edges(
    "guardrail",
    route_after_guardrail,
    {
        "continue": "resolve_location",  # ì •ìƒ ì§„í–‰
        "terminate": END  # ì¦‰ì‹œ ì¢…ë£Œ
    }
)
workflow.add_edge("resolve_location", "load_memories")
workflow.add_edge("load_memories", "agent")
workflow.add_conditional_edges("agent", should_continue, {
    "tools": "tools",
    "adjust_params": "adjust_params",
    END: "sync_db"
})
workflow.add_edge("tools", "agent")
workflow.add_edge("adjust_params", "agent")
workflow.add_edge("sync_db", END)

graph = workflow.compile()
