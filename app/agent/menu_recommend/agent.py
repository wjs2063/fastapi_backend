import operator
from typing import Annotated, Literal, TypedDict

from langchain.agents import create_agent
from langchain.messages import AIMessage, HumanMessage
from langchain_core.messages import (
    BaseMessage,
    SystemMessage,
    ToolMessage,
    message_to_dict,
    messages_from_dict,
)
from langchain_core.runnables import RunnableConfig
from langchain_openai import ChatOpenAI
from langgraph.graph import END, START, StateGraph
from langgraph.store.postgres import AsyncPostgresStore
from pydantic import BaseModel, Field

from app.agent.menu_recommend.tools import NaverLocalSearchTool
from app.clients.naver import naver_map_client


class UserPreference(BaseModel):
    """ì‚¬ìš©ìì˜ ê°œë³„ ì·¨í–¥ ì •ë³´"""

    category_path: str = Field(
        description="ê³„ì¸µì  ì¹´í…Œê³ ë¦¬ ê²½ë¡œ. 'ëŒ€ë¶„ë¥˜ > ì¤‘ë¶„ë¥˜ > ì†Œë¶„ë¥˜' í˜•ì‹ (ì˜ˆ: 'ìŒì‹ > ì¼ì‹ > ì´ˆë°¥')"
    )
    preference_type: Literal["LIKES", "DISLIKES", "ALLERGIC_TO"] = Field(
        description="ì·¨í–¥ ìœ í˜•: ì¢‹ì•„í•¨(LIKES), ì‹«ì–´í•¨(DISLIKES), ì•ŒëŸ¬ì§€(ALLERGIC_TO)"
    )
    domain: Literal["FOOD", "TRAVEL", "LIFESTYLE"] = Field(
        default="FOOD", description="ì·¨í–¥ì˜ ë„ë©”ì¸ ë¶„ì•¼"
    )
    reason: str | None = Field(default=None, description="ì·¨í–¥ì˜ ì´ìœ ë‚˜ êµ¬ì²´ì ì¸ ì„¤ëª…")


class PreferenceList(BaseModel):
    """ì¶”ì¶œëœ ì·¨í–¥ ì •ë³´ì˜ ë¦¬ìŠ¤íŠ¸ ì»¨í…Œì´ë„ˆ"""

    preferences: list[UserPreference] = Field(
        description="ì‚¬ìš©ì ëŒ€í™”ì—ì„œ ì¶”ì¶œëœ ëª¨ë“  ì·¨í–¥ ì •ë³´ ë¦¬ìŠ¤íŠ¸"
    )


class UserInfo(BaseModel):
    lat: float | None = None
    lng: float | None = None


# --- State ì •ì˜ ---
class AgentState(TypedDict):
    user_id: str
    request_id: str
    query: str
    history: list[BaseMessage]
    context: str
    search_params: dict[str, int]  # ë™ì  í˜ì´ì§•ìš©
    has_results: bool
    user_info: UserInfo
    answer: AIMessage
    address: str | None  # ë³€í™˜ëœ ì£¼ì†Œ (ì˜ˆ: "ê°•ë™êµ¬ ì²œí˜¸ë™")
    is_related: bool  # ê°€ë“œë ˆì¼ í†µê³¼ ì—¬ë¶€ í”Œë˜ê·¸
    internal_steps: Annotated[list[str | BaseMessage | ToolMessage], operator.add]


class RelevanceCheck(BaseModel):
    """ì§ˆë¬¸ì˜ ê´€ë ¨ì„± íŒë³„ ê²°ê³¼"""

    is_related: bool = Field(description="ìŒì‹/ë§›ì§‘ ê´€ë ¨ ì§ˆë¬¸ ì—¬ë¶€")
    reason: str = Field(
        description="ì´ìœ ë¥¼ í•œê¸€ë¡œ ì„¤ëª… (ì˜ˆ: 'ìŒì‹ê³¼ ê´€ë ¨ ì—†ëŠ” ì¼ìƒ ëŒ€í™”ì…ë‹ˆë‹¤')"
    )


async def fetch_chat_history(state: AgentState, config: RunnableConfig):
    store: AsyncPostgresStore = config["configurable"]["store"]

    items = await store.asearch((state["user_id"],))
    items.sort(key=lambda x: x.updated_at, reverse=True)
    history = []

    for item in items:
        print(item)
        history.extend(messages_from_dict(item.value["history"]))
    return {"history": history}


async def guardrail(state: AgentState, config: RunnableConfig):
    """ìµœê·¼ 5ìŒ(10ê°œ)ì˜ ëŒ€í™”ë¥¼ ë¶„ì„í•˜ì—¬ ì§ˆë¬¸ì˜ ê´€ë ¨ì„±ì„ íŒë³„í•©ë‹ˆë‹¤."""

    # 1. ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ì—ì„œ ìµœê·¼ 10ê°œë§Œ ìŠ¬ë¼ì´ì‹± (Human-AI ëŒ€í™” ì•½ 5ìŒ)
    # messagesê°€ 10ê°œë³´ë‹¤ ì ì–´ë„ íŒŒì´ì¬ ìŠ¬ë¼ì´ì‹±ì€ ì—ëŸ¬ ì—†ì´ ìˆëŠ” ë§Œí¼ë§Œ ê°€ì ¸ì˜µë‹ˆë‹¤.

    recent_context = state["history"]

    if not recent_context:
        return {"is_related": True}

    # 2. LLM ì„¤ì • (Structured Output)
    llm = ChatOpenAI(
        model="gpt-4o-mini",
        temperature=0,
        max_tokens=300,  # ê°€ë“œë ˆì¼ì€ ì§§ì€ ì‘ë‹µì´ë©´ ì¶©ë¶„í•˜ë¯€ë¡œ ì¤„ì„
    ).with_structured_output(RelevanceCheck)

    # 3. ë§¥ë½ ê¸°ë°˜ íŒë³„ ìš”ì²­
    try:
        check = await llm.ainvoke(
            [
                SystemMessage(
                    content="""ë‹¹ì‹ ì€ ë§›ì§‘ ì¶”ì²œ ì„œë¹„ìŠ¤ì˜ ë³´ì•ˆ ê°€ë“œì…ë‹ˆë‹¤.
            ì œê³µëœ 'ìµœê·¼ ëŒ€í™” ë§¥ë½'ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ë§ˆì§€ë§‰ ì§ˆë¬¸ì´ ì„œë¹„ìŠ¤ ë²”ìœ„(ìŒì‹, ë§›ì§‘, ì‹ë‹¹, ìš”ë¦¬, ì·¨í–¥)ì— ì†í•˜ëŠ”ì§€ íŒë³„í•˜ì„¸ìš”.

            [íŒë‹¨ ë¡œì§]
            1. ë§¥ë½ ìš°ì„ : ì§ˆë¬¸ ìì²´ì— 'ìŒì‹' ë‹¨ì–´ê°€ ì—†ì–´ë„, ì´ì „ ëŒ€í™”ê°€ ë§›ì§‘ ì¶”ì²œ ì¤‘ì´ì—ˆê³  "ê±°ê¸°ëŠ” ì–´ë•Œ?", "ë‹¤ë¥¸ ë°ëŠ”?" ê°™ì€ ì§ˆë¬¸ì´ë¼ë©´ ê´€ë ¨ ìˆìŒ(True)ì…ë‹ˆë‹¤.
            2. ì¸ì‚¬ ë° ì¢…ë£Œ: "ì•ˆë…•", "ê³ ë§ˆì›Œ", "ì˜ ê°€" ë“± ê¸°ë³¸ì ì¸ ëŒ€í™” ì˜ˆì ˆì€ ê´€ë ¨ ìˆìŒ(True)ìœ¼ë¡œ ê°„ì£¼í•©ë‹ˆë‹¤.
            3. ì£¼ì œ ì´íƒˆ: ê°‘ì‘ìŠ¤ëŸ¬ìš´ ì½”ë”© ì§ˆë¬¸, ì •ì¹˜, ì¼ë°˜ ìƒì‹ ë“±ì€ ê´€ë ¨ ì—†ìŒ(False)ì…ë‹ˆë‹¤.

            ë°˜ë“œì‹œ í•œê¸€ë¡œ ì´ìœ (reason)ë¥¼ ì§§ê²Œ í¬í•¨í•˜ì—¬ JSONìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”."""
                ),
                # ìµœê·¼ 10ê°œì˜ ëŒ€í™” ë‚´ìš©ì„ ëª¨ë‘ ì „ë‹¬
                *recent_context,
                HumanMessage(content=state["query"]),
            ]
        )

        if not check.is_related:
            rejection_msg = AIMessage(
                content="ğŸ• ì €ëŠ” ìŒì‹ê³¼ ë§›ì§‘ì— ëŒ€í•´ì„œë§Œ ì´ì•¼ê¸°í•  ìˆ˜ ìˆëŠ” ì „ë¬¸ê°€ì˜ˆìš”! ë¨¹ê³  ì‹¶ì€ ë©”ë‰´ë‚˜ ë§›ì§‘ ì·¨í–¥ì— ëŒ€í•´ ë¬¼ì–´ë´ ì£¼ì‹œê² ì–´ìš”?"
            )
            # ê°€ë“œë ˆì¼ì— ê±¸ë¦¬ë©´ ì¤‘ë‹¨ í”Œë˜ê·¸ì™€ ê±°ì ˆ ë©”ì‹œì§€ ë°˜í™˜
            return {"answer": rejection_msg, "is_related": False}

    except Exception:
        # ì—ëŸ¬ ë°œìƒ ì‹œ ì‚¬ìš©ì ê²½í—˜ì„ ìœ„í•´ ì¼ë‹¨ í†µê³¼ì‹œí‚¤ëŠ” Fallback
        return {"is_related": True}

    return {"is_related": True}


# 2. ì¡°ê±´ë¶€ ì—£ì§€ í•¨ìˆ˜
def route_after_guardrail(state: AgentState, config: RunnableConfig):
    """
    is_relatedê°€ Falseë©´ ë°”ë¡œ ì¢…ë£Œ(END) ì‹œê·¸ë„ì„ ë³´ëƒ…ë‹ˆë‹¤.
    """
    if state.get("is_related") is False:
        # ê´€ë ¨ ì—†ëŠ” ì§ˆë¬¸ì€ ë’¤ë„ ì•ˆ ëŒì•„ë³´ê³  END!
        return "terminate"
    return "continue"


async def resolve_location(state: AgentState, config: RunnableConfig):
    """ì¢Œí‘œê°€ ìˆë‹¤ë©´ ì£¼ì†Œë¡œ ë³€í™˜í•˜ì—¬ stateì— ì €ì¥"""
    lat = state["user_info"].lat
    lng = state["user_info"].lng
    if lat and lng:
        map_client = naver_map_client
        address = await map_client.get_address(lat=lat, lng=lng)
        return {"address": address}
    return {"address": None}


# --- ë…¸ë“œ êµ¬í˜„ ---
async def load_preference(state: AgentState, config: RunnableConfig):
    db = config["configurable"].get("neo4j_service")
    prefs = await db.get_user_context(state["user_id"])

    formatted = (
        "\n".join(
            [f"- {' > '.join(p['category_path'])}: {p['preference_type']}" for p in prefs]
        )
        if prefs
        else "ì·¨í–¥ ì •ë³´ ì—†ìŒ"
    )
    return {
        "context": formatted,
        "search_params": {"start": 1, "display": 5, "retry_count": 0},
    }


async def call_agent(state: AgentState, config: RunnableConfig):
    # 1. ëª¨ë¸ê³¼ ë„êµ¬ ì„¤ì •
    llm = ChatOpenAI(model="gpt-4o", temperature=0)
    # create_react_agentëŠ” ë„êµ¬ í˜¸ì¶œ -> ì‹¤í–‰ -> ê²°ê³¼ í™•ì¸ -> ë‹µë³€ ë£¨í”„ë¥¼ ìë™ìœ¼ë¡œ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    agent = create_agent(llm, tools=[NaverLocalSearchTool()])

    clean_history = []
    for i, msg in enumerate(state["history"]):
        if isinstance(msg, AIMessage) and msg.tool_calls:
            # ë‹¤ìŒ ë©”ì‹œì§€ë“¤ì´ ì´ ë„êµ¬ í˜¸ì¶œë“¤ì— ëŒ€í•œ ì‘ë‹µì¸ì§€ í™•ì¸
            has_tool_responses = any(
                isinstance(next_msg, ToolMessage)
                for next_msg in state["history"][i + 1 :]
            )
            if not has_tool_responses:
                # ê²°ê³¼ê°€ ì—†ë‹¤ë©´ tool_callsë¥¼ ì œê±°í•˜ì—¬ ì¼ë°˜ ë©”ì‹œì§€ë¡œ ë³€í™˜ (OpenAI ì—ëŸ¬ ë°©ì§€)
                msg = AIMessage(content=msg.content, tool_calls=[])
        clean_history.append(msg)

    # 2. ì‹œìŠ¤í…œ ì§€ì¹¨ êµ¬ì„± (íˆìŠ¤í† ë¦¬ëŠ” ì—¬ê¸°ì„œ ì œì™¸)
    loc_context = f"\n[í˜„ì¬ ìœ„ì¹˜] {state['address']}" if state.get("address") else ""
    system_content = f"""ë‹¹ì‹ ì€ ë§›ì§‘ ì¶”ì²œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
    [ì‚¬ìš©ì ì·¨í–¥ ì •ë³´]
    {state["context"]}
    {loc_context}

    ì§€ì¹¨:
    1. ì‚¬ìš©ìì˜ í˜„ì¬ ìœ„ì¹˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë§›ì§‘ì„ ê²€ìƒ‰í•˜ì„¸ìš”.
    2. 'naver_local_search' ë„êµ¬ë¥¼ í˜¸ì¶œí•  ë•Œ, ì¿¼ë¦¬ì— ë°˜ë“œì‹œ ì§€ì—­ëª…ê³¼ ë©”ë‰´ë¥¼ í¬í•¨í•˜ì„¸ìš”.
       ì˜ˆ: "{state.get("address", "")} ì´ˆë°¥ ë§›ì§‘"
    3. ê²°ê³¼ê°€ ë§Œì¡±ìŠ¤ëŸ½ì§€ ì•Šìœ¼ë©´ ê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ ë°”ê¿”ì„œ ì¬ì‹œë„í•˜ì„¸ìš”.
    4. ëª¨ë“  ê²€ìƒ‰ ê²°ê³¼ê°€ ë‚˜ì˜¤ë©´ ìµœì¢…ì ìœ¼ë¡œ ì‚¬ìš©ìì—ê²Œ ì¹œì ˆí•˜ê²Œ ë§›ì§‘ì„ ì¶”ì²œí•˜ì„¸ìš”."""
    input_messages = (
        [SystemMessage(content=system_content)]
        + clean_history
        + [
            SystemMessage(
                content="ì‚¬ìš©ìì™€ì˜ ëŒ€í™”ê¸°ë¡ì„ ê¸°ë°˜ìœ¼ë¡œ ë§¥ë½ì„ íŒŒì•…í•˜ì—¬, ë‹¤ë¥¸ ì‹ë‹¹ì„ ì¶”ì²œí•´ì•¼í• ì§€, "
                "ë‹¤ë¥¸ ìŒì‹ì¹´í…Œê³ ë¦¬ë¥¼ ì¶”ì²œí•´ì•¼í• ì§€ ë“±ì„ íŒŒì•…í•˜ì—¬ ììœ¨ì ìœ¼ë¡œ ëŒ€ë‹µí•˜ì„¸ìš”"
            )
        ]
        + [HumanMessage(content=state["query"])]
    )
    # 4. ì—ì´ì „íŠ¸ ì‹¤í–‰ (ììœ¨ ë£¨í”„ ì‹œì‘)
    # ainvokeëŠ” ëª¨ë“  ë„êµ¬ í˜¸ì¶œ ë‹¨ê³„ê°€ ëë‚  ë•Œê¹Œì§€ ë‚´ë¶€ì ìœ¼ë¡œ ë°˜ë³µ ì‹¤í–‰ë©ë‹ˆë‹¤.
    result = await agent.ainvoke({"messages": input_messages}, config)

    # 5. ê²°ê³¼ ë°˜í™˜
    # result["messages"]ì˜ ë§ˆì§€ë§‰ ìš”ì†Œê°€ ëª¨ë“  ë£¨í”„ë¥¼ ë§ˆì¹œ LLMì˜ ìµœì¢… ë‹µë³€ì…ë‹ˆë‹¤.
    final_answer = result["messages"][-1]

    return {
        "answer": final_answer,
        "internal_steps": result[
            "messages"
        ],  # ë„êµ¬ í˜¸ì¶œ ê³¼ì • ì „ì²´ë¥¼ ë³´ê´€í•˜ê³  ì‹¶ì„ ë•Œ ì‚¬ìš©
    }


def adjust_params(state: AgentState, config: RunnableConfig):
    curr = state["search_params"]
    return {
        "search_params": {
            "start": curr["start"] + 5,
            "display": 5,
            "retry_count": curr["retry_count"] + 1,
        }
    }


async def sync_db(state: AgentState, config: RunnableConfig):
    """ì‚¬ìš©ìì˜ ë§ˆì§€ë§‰ ë©”ì‹œì§€ì—ì„œë§Œ ì·¨í–¥ì„ ì¶”ì¶œí•˜ì—¬ Neo4jì— ì €ì¥"""
    db = config["configurable"].get("neo4j_service")
    store: AsyncPostgresStore = config["configurable"]["store"]
    # 1. ë©”ì‹œì§€ ê¸°ë¡ ì¤‘ ì‚¬ìš©ìê°€ ë³´ë‚¸ ê²ƒë§Œ í•„í„°ë§
    user_messages = [HumanMessage(content=state["query"])] + [
        m for m in state["internal_steps"] if isinstance(m, HumanMessage)
    ]

    # ì‚¬ìš©ìì˜ ë©”ì‹œì§€ê°€ ì—†ìœ¼ë©´(ê·¸ëŸ´ ë¦¬ ì—†ê² ì§€ë§Œ ë°©ì–´ì½”ë“œ) ì¢…ë£Œ
    if not user_messages:
        return {}

    # ê°€ì¥ ìµœê·¼ì˜ ì‚¬ìš©ì ë©”ì‹œì§€ ì„ íƒ
    last_user_query = user_messages[-1].content

    # 2. Structured Output ì„¤ì •
    llm = ChatOpenAI(
        model="gpt-4o",
        temperature=0,
        max_tokens=500,
    ).with_structured_output(PreferenceList)

    # 3. ì¶”ì¶œ ìˆ˜í–‰ (ì‚¬ìš©ìì˜ ë°œí™” ë‚´ìš©ë§Œ ì „ë‹¬)
    extracted = await llm.ainvoke(
        [
            SystemMessage(
                content="ì‚¬ìš©ìì˜ ë°œí™”ì—ì„œ ìŒì‹/ì—¬í–‰ ì·¨í–¥ì„ 'ëŒ€ë¶„ë¥˜ > ì¤‘ë¶„ë¥˜ > ì†Œë¶„ë¥˜' ê²½ë¡œë¡œ ì¶”ì¶œí•˜ì„¸ìš”. ì¶”ì²œ ê²°ê³¼ê°€ ì•„ë‹Œ ì‚¬ìš©ìì˜ ì‹¤ì œ ì„ í˜¸ë„ë§Œ ì¶”ì¶œí•´ì•¼ í•©ë‹ˆë‹¤."
            ),
            HumanMessage(content=last_user_query),
        ]
    )

    # 4. DB ì €ì¥
    if extracted and extracted.preferences:
        for p in extracted.preferences:
            await db.upsert_hierarchical_preference(
                state["user_id"], p.category_path, p.preference_type, p.domain
            )
    await store.aput(
        (state["user_id"], state["request_id"]),
        key="chat",
        value={
            "history": [
                message_to_dict(HumanMessage(content=state["query"])),
                message_to_dict(state["answer"]),
            ]
        },
    )
    return {}


# --- ê·¸ë˜í”„ êµ¬ì¶• ---
workflow = StateGraph(AgentState)
workflow.add_node("fetch_chat_history", fetch_chat_history)
workflow.add_node("resolve_location", resolve_location)
workflow.add_node("load_memories", load_preference)
workflow.add_node("agent", call_agent)
workflow.add_node("sync_db", sync_db)
workflow.add_node("guardrail", guardrail)


workflow.add_edge(START, "fetch_chat_history")
workflow.add_edge("fetch_chat_history", "guardrail")
# ê°€ë“œë ˆì¼ ê²°ê³¼ì— ë”°ë¥¸ ë¶„ê¸° ë¡œì§ ì—…ë°ì´íŠ¸
workflow.add_conditional_edges(
    "guardrail",
    route_after_guardrail,
    {
        "continue": "resolve_location",  # ì •ìƒ ì§„í–‰
        "terminate": END,  # ì¦‰ì‹œ ì¢…ë£Œ
    },
)
workflow.add_edge("resolve_location", "load_memories")
workflow.add_edge("load_memories", "agent")
workflow.add_edge("agent", "sync_db")
workflow.add_edge("sync_db", END)
